# Linux内核设计与实现


> 书中内容基于 `kernel-2.6.34` 版本内核，书籍名《Linux内核设计与实现》

## Linux内核简介

### kernel许可协议

linux kernel 基于 (GPL v2)作为限制条款，保障每个人都可以自由获取、修改内核，但是也要让得到你修改内核的人同时也享有你曾经享有的权利——开源全部源代码。

## 从内核出发

### 内核源码树

|目录|描述|
|:---|:---|
|arch|特定体系结构的源码|
|block|块设备I/O层|
|crypto|加密API|
|Documentation|内核源码文档|
|drivers|设备驱动程序|
|firmware|使用某些驱动程序而需要的设备固件|
|fs|VFS和各种文件系统|
|include|内核头文件|
|init|内核引导和初始化|
|ipc|进程间通信代码|
|kernel|像调度程序这样的核心子系统|
|lib|通用内核函数|
|mm|内存管理子系统和VM|
|net|网络子系统|
|samples|示例代码|
|scripts|编译内核所用的脚本|
|security|Linux安全模块|
|sound|语音子系统|
|usr|早期用户空间代码(所谓的initramfs)|
|tools|在linux开发中有用的工具|
|virt|虚拟化基础结构|

### 配置内核

1. `make config`
2. `make menuconfig`
3. `make gconfig`
4. `make defconfig`：基于默认配置来编译内核
5. `make oldconfig`：基于已有配置来编译内核

### 编译内核
### 安装内核
### 安装内核模块
### 生成initramfs
### 更新引导

> 编译内核时候会在内核代码根目录下创建一个 System.map 文件，这是一份符号对照表，用来将内核符号和他们的起始地址对应起来，调试时候用来把内存地址翻译成容易理解的函数名和变量名。

### 内核开发的特点

- 内核编程时候既不能访问C库，也不能访问标准的C头文件(常用C库函数在linux内核中已经被实现)
- 内核编程时候必须使用 GNU C。
- 内核编程时缺乏用户空间那样的内存保护机制。
- 内核编程时候难以执行浮点运算。
- 内核给每个进程只有一个很小的定长堆栈。
- 由于内核支持异步中断、抢占和SMP，因此必须时刻注意同步和并发。
- 要考虑可移植性

### Kernel头文件使用

- 基本的头文件位于内核源码顶级目录下的`include` 目录中。例如：头文件<linux/inotify.h>对应内核源码树中的`include/linux/inotify.h`
- 体系结构相关的头文件集位于内核源代码树的 `arch/<archiecture>/include/asm`目录下。梅核代码通过以 `asm/`为前缀的方式包含这些头文件，例如：<asm/ioctl.h>

### kernel 打印日志

内核中无`printf()`函数，但是它提供了`printk()`，`printk()`负责把格式化好的字符串拷贝到内核日志缓冲区上，`printf()`和`printk()`之间的一个显著区别在于，`printk()`允许你通过指定一个标志来设置优先级。syslogd会根据这个优先级标志来决定在什么地方显示这条系统信息。
```
printk(KERN_ERR "this is an error\n");
```

### GNU C

Linux内核是用C语言编写的，但是内核不完全符合ANSIC标准，只要有可能，内核开发者总是要用到 gcc 提供的多语言扩展部分(gcc是多种GNU编译器的集合，它包含的C编译器既可以编译内核，也可以编译Linux系统上用C语言写的其它代码)。
<br/>

内核开发者使用的C语言涵盖了ISO C99标准和GNU C扩展特性，最早只有 gcc 提供足够多的扩展特性，才可以用来编译linux内核。
<br/>

以下是一些内核代码中使用到的C语言扩展部分：
#### 内联函数(inline)

C99 和 GNU C 都支持内联函数。函数会在调用位置展开，消除函数调用和返回带来的开销(寄存器存储和恢复)。不过也有缺点，这会导致代码变长，意味着占用更多的内存空间或者占用更多的指令缓存。内核开发者通常把那些对事件要求比较高，本身长度又比较短的函数定义成内联函数。如果一个函数较大，会被反复调用，且没有特别的时间上的限制，我们不赞成把它做成内联函数。

定义一个内联函数时候，需要使用`static`作为关键字，并且用`inline`限定它。比如：`static inline void wolf(unsigned long tail_size);`

内核中，为了类型安全和易读性，有限使用内联函数，而不是复杂的宏。

#### 汇编内联

gcc 编译器支持在C函数中嵌入汇编指令。当然在内核编程的时候，只有知道对应的体系结构才能使用这个功能。

通常使用 asm() 指令嵌入汇编代码，例如：
```c
unsigned int low, high; // low 和 high 分别包含64位时间戳的低32位和高32位
asm volatile("rdtsc" : "=a" (low), "=d" (high));
```

Linux 内核混合使用了C语言和汇编语言。在偏近体系结构的底层或对执行事件要求严格的地方，一般使用的是汇编语言。而内核其它部分的大部分代码是用C语言编写的。

#### 分支声明

对于条件选择语句，gcc内建了一条指令用于优化，在一个条件经常出现，或者该条件很少出现的时候，编译器可以根据这条指令对条件分支选择进行优化。内核把这条指令封装成了宏，比如`likely()`和`unlikely()`，这样使用起来比较方便。
<br/>

例如下面是一个条件选择语句：
```c
if (error) {
    /* ... */
}
```
如果想要把这个选择标记成绝少发生的分支：
```c
// 我们认为 error 绝大部分事件都会是0
if (unlikely(error)) {
    /* ... */
}
```

如果想要把一个分支标记位通常为真的选择：
```c 
// 我们认为 success 通常都不会为0
if (likely(success)) {
    /* ... */
}
```
> 在使用这两个宏之前，一定要搞清楚这个条件，如果判断不准确，那么性能反而会下降。

#### 没有内存保护机制

如果用户程序访问非法内存，内核会发现这个错误，发送 SIGSEGV 信号，并结束整个进程。

如果内核自己非法访问内存，就会导致 oops，且结果很难控制。

另外内核中的内存并不分页，也就是说，你每用掉一个字节，物理内存就减少一个字节。所以，在你想往内核里加入新功能时候要记住这一点。

#### 不要轻易在内核中使用浮点数

在用户空间的进程内使用浮点操作的时候，内核会完成从整数操作模式到浮点数操作模式转换。在执行浮点指令时候因体系结构不同，内核的选择也不同，但是，内核通常捕获陷阱并着手于整数到浮点方式的转变。

与用户空间进程不同，内核并不能完美地支持浮点操作，因为它本身不能陷入。在内核中使用浮点数时候，除了要人工保存和恢复浮点寄存器，还有其它一些琐碎的事情要做。如果要直截了当的回答，那就是：别这样做，除非一些极少情况，否则别在内核中使用浮点操作。

#### 容积小而固定的栈

用户空间的程序可以从栈上分配大量的空间来存放变量，甚至巨大的结构体或者是包含数以千计的数据项的数组都没问题。之所以可以这么做，是因为用户空间的栈本身比较打，而且还能动态增长。

内核堆栈的准确大小随体系结构而变。在 X86 上，栈的大小在编译时候配置，可以是 4KB 也可以是 8KB。

历史上说，内核栈的大小是两页，32位机器内核栈是 8KB，而 64位机器是 16KB，这个固定不变的，每个处理器都有自己的栈。

#### 同步和并发

内核很容易产生竞争条件。和单线程的用户空间程序不同，内核的许多特性都要求能够并发地访问共享数据，这就要求有同步机制以保证不出现竞争条件，特别是：

- Linux是抢占多任务操作系统。内核的进程调度程序即兴对进程进行调度和重新调度。内核必须和这些任务同步。
- Linux内核支持对称多处理器系统(SMP)。所以，如果没有适当的保护，同时在两个或两个以上的处理器上执行的内核代码很可能会同时访问共享的同一个资源。
- 中断是异步到来的，完全不顾及当前正在执行的代码。也就是说，如果不加以适当的保护，中断完全有可能在代码访问资源的时候到来，这样，中断处理程序就有可能访问同一资源。
- Linux内核可以抢占。所以，如果不加以适当的保护，内核中一段正在执行的代码可能会被另外一段代码抢占，从而有可能导致几段代码同时访问相同的资源。

常用的解决竞争的办法是`自旋锁`和`信号量`。

#### 可移植性的重要性

大部分 C 代码应该与体系结构无关，在许多不同体系结构的计算机上都能编译和执行，因此，必须把与体系结构相关的代码从内核代码树的特定目录中适当地分离出来。

诸如保持字节序、64位对齐、不假定字长和页面长度等一系列准则都有助于移植性。

## 进程管理

- 进程定义和相关概念，比如：线程
- Linux内核如何管理进程
- 进程在内核中的生命周期

**进程管理是整个操作系统的心脏所在**

### 进程

进程是处于执行期的程序(目标码存放在某种介质上)。但进程并不仅仅局限于一段可执行程序代码(Unix称其为代码段，text section)。通常进程还要包含其它资源，
像打开的文件、挂起的信号，内核内部数据，处理器状态，一个或多个具有内存映射的内存地址空间及一个或多个执行线程，当然还包括用来存放全局变量的数据段等。
实际上，进程就是正在执行的程序代码的实时结果。内核需要有效而又透明的管理所有细节。

执行线程，简称线程，是在进程中活动的对象。每个线程都拥有一个独立的程序计数器、进程栈和一组进程寄存器。内核调度的对象是线程，而不是进程。
在传统的Unix系统中，一个进程只包含一个线程，但现在的系统中，包含多个线程的多线程程序司空见惯。对Linux而言，并不特别区分线程和进程。
<br/>

在现代操作系统中，进程提供两种虚拟机制：**虚拟处理器**和**虚拟内存**。虽然实际上可能是许多进程正在分享一个处理器，但是虚拟处理器给进程一个假象，
让这些进程觉得自己在独享处理器。
<br/>

程序不是进程，进程是处于执行期间程序及其相关资源的总称。
<br/>

Linux通过调用 `fork()` 复制一个现有进程来创建一个全新的进程。调用 `fork()` 的进程称为父进程，新产生的进程称为 `子进程`。在调用结束时候，
在返回点这个相同位置上，父进程恢复执行，子进程开始执行。 `fork()` 系统调用从内核返回两次：一次回到父进程，另一次回到新产生的子进程。
<br/>

通常，创建新的进程都是为了立即执行新的、不同的程序，接着调用 `exec()` 这组函数就可以创建新的地址空间，并把新的程序载入其中。
在现代 Linux 内核中， `fork()` 实际上是由 `clone()` 系统调用完成的，后者将在后面讨论。
<br/>

最终，程序通过 `exit()` 系统调用退出执行。这个函数会终结进程并将其占用的资源释放掉。父进程可以通过 `wait()` 系统调用查询子进程是否终结，
这其实使得进程拥有了等待特定进程执行完毕的能力。进程退出执行后被设置为僵死状态，直到它的父进程调用 `wait()` 或者 `waitpid()` 为止。

> 注意：进程的另一个名字是 `任务`。Linux 内核通常把进程也叫做任务，两个术语是一样的。

### 进程描述符及任务结构

内核把进程的列表存放在叫做任务队列(task list)的双向循环列表中。链表中的每一项都是类型为 `task_struct`、称为进程描述符的结构，该结构定义在
`<linux/sched.h>` 文件中。进程描述符中包含一个具体进程的所有信息。
<br/>

`task_struct` 相对较大，在 32 位机器上，它大约有 1.7KB。但如果考虑到该结构内包含了内核管理一个进程所需的所有信息，那么它的大小也就算
相当小了。进程描述符中包含的数据能完整描述一个正在执行的程序：此进程打开的文件、进程的地址空间、挂起的信号、进程的状态、还有其它更多信息。

#### 分配进程描述符

Linux 通过 `slab` 分配器分配 `task_struct` 结构，这样能达到对象复用和缓存着色的目的。在 2.6 以前的内核中，各个进程的 `task_struct` 
存放在它们内核栈的尾端。这样做是为了让那些像X86那样寄存器较少的硬件体系结构只要通过栈指针就能计算出它的位置，从而避免使用额外的寄存器专门记录。
由于现在用 slab 分配器动态生成 `task_struct`，所以只需在栈底(对于向下增长的栈来说)或栈顶(对于向上增长的栈来说)创建一个新的结构 `struct thread_info`
在 x86 上， `struct thread_info`在文件 `<asm/thread_info.h>`中定义如下：
```c
struct thread_info {
    struct task_struct*         task;
    struct exec_domain*         exec_domain;
    __u32                       flags;
    __u32                       status;
    __u32                       cpu;
    int                         preempt_count;
    mm_segment_t                addr_limit;
    struct restart_block        restart_block;
    void*                       sysenter_return;
    int                         uaccess_err;
};
```
<div align=center><img src='/pic/linux/kernel-process-1.png'/></div>
<center>进程描述符和内核栈</center>

每个任务的 `thread_info` 结构在它的内核栈的末尾端分配。结构中 task 域中存放的是指向该任务实际 `task_struct` 的指针。

> 通过预先分配和重复使用 `task_struct`，可以避免动态分配和释放带来的资源消耗。

>寄存器较弱的体系结构不是引入 `thread_info` 结构的唯一原因。这个新建的结构使在汇编代码中计算其偏移变得非常容易。

#### 进程描述符的存放

内核通过一个唯一的进程标识值(process identification value) 或 PID 来标识每个进程。PID 是一个数，标识为 pid_t 隐含类型(隐含类型标识隐藏了该类型实现，用句柄替代类型实现)，
实际上就是一个int类型。为了与老版本的Unix和Linux兼容，PID 的最大值默认设置为 32768 (short int 短整型的最大值)，当然这个值也可以增加到
400万(受 `<linux/threads.h>` 中所定义PID最大值的限制)。

内核把每个进程 PID 存放在它们各自的进程描述符中。

> PID 最大值表明了系统中允许同时存在的进程的最大数目。32768一般来说够用了，而且这个值越小，进程调度一圈就越快。

在内核中，访问任务/进程通常需要获得指向其 `task_struct` 的指针。实际上，内核中大部分处理进程的代码都是直接通过
`task_struct` 进行的。因此，通过 `current` 宏查找到当前正在运行进程的进程描述符的速度就显得尤为重要。硬件体系结构不同，
该宏的实现也不同，它必须针对专门的硬件体系结构做处理。有的硬件体系结构可以拿出一个专门寄存器来存放指向当前进程
`task_struct` 的指针，用于加快访问速度。而有些像x86这样的体系结构(其寄存器并不富余)，就只能在内核栈的尾端创建 
`thread_info` 结构，通过计算偏移间接地查找 `task_struct` 结构。

在 X86 系统上，current 把栈指针的后 13 个有效位屏蔽掉，用来计算出 `thread_info` 的偏移。该操作是通过 `current_thread_info()`
函数完成的。汇编代码如下：
```asm
mov1 $-8192, %eax
andl %esp, %eax
```
> 这里假定栈的大小为 8KB。当 4KB 的栈启用时候，就要用 4096，而不是 8192.

最后，current再从 `thread_info` 的 `task` 域中提取并返回 `task_struct` 的地址：
```c
current_thread_info()->task;
```

> 对比一下这部分在 PowerPC (IBM基于RISC 的现代微处理器)，我呢把可以发现 PPC 当前的 `task_struct` 是保存在一个寄存器中。
> 也就是说，在 PPC 上， current 宏只需要把 r2 寄存器中的值返回就行了。与 X86 不一样，PPC 有足够多的寄存器，所以它的实现有这样选择的余地。
> 而访问进程描述符是一个重要的频繁操作，所以 PPC 的内核开发者觉得完全有必要为此使用一个专门的寄存器。

#### 进程状态

进程描述符中的 `state` 域描述了进程的当前状态。系统中的每个进程都必然处于五种进程状态中的一种。该域的值也必为以下五种状态标识之一：

- TASK_RUNNING(运行) —— 进程是可执行的：它或者正在执行，或者在运行队列中等待执行。这是进程在用户空间中执行的唯一可能的状态；这种状态也可以应用到内核空间中正在执行的进程。
- TASK_INTERRUPTIBLE(可中断) —— 进程正在睡眠(也就是说它被阻塞)，等待某些条件的达成。一旦这些条件达成，内核就会把
进程状态设置为可运行。处于此状态的进程也会因为接收到信号而提前被唤醒并随时准备投入运行。
- TASK_UNINTERRUPTIBLE(不可中断) —— 除了就算是接收到信号也不会被唤醒或者准备投入运行外，这个状态与可打断状态相同。这个状态通常在进程必须等待时不受干扰或等待事件很快就会发生时候出现。由于处于此状态的任务对信号不做响应，所以较之可中断状态使用的比较少。
- __TASK_TRACED —— 被其它进程跟踪的进程，例如通过 `ptrace` 对调试程序进行跟踪。
- __TASK_STOPPED(停止) —— 进程停止执行；进程没有投入运行也不能投入运行。通常这种状态发生在接收到 `SIGSTOP`、`SIGTSTP`、`SIGTTOU` 等信号的时候，此外在调试期间收到任何信号，都会使进程进入这种状态。

<div align=center><img src='/pic/linux/kernel-process-2.png'/></div>
<center>进程状态转化</center>

#### 设置当前进程状态

内核经常需要调整某个进程的状态。这时最好使用 `set_task_state(task, state)` 函数：
```c
set_task_state(task, state);    // 将任务 task 的状态设置为 state
```

该函数将指定的进程设置为指定状态。必要的时候下，它会设置内存屏障来强制其它处理器做重新排序(一般只有在 SMP 系统中有此必要)。否则，它等价于：
```c
task->state = state;
```

> set_current_state(state) 和 set_task_state(current, state) 含义是等同的。参看 `<linux/sched.h>` 中相关函数的说明。

#### 进程上下文

可执行程序代码是进程的重要组成部分。这些代码从一个可执行文件载入到进程的地址空间执行。一般程序在用户空间执行。当一个程序执行了系统调用或者触发了某个异常，它就陷入了内核空间。
此时，我们称内核 “代表进程执行”并处于进程上下文中。在此上下文中 `current` 宏是有效的。除非在此间隙有更高优先级的进程需要执行并由调度器做出了相应调整，否则在内核退出的时候，程序恢复在用户空间会继续执行。

系统调用和异常处理程序是对内核明确定义的接口。进程只有通过这些接口才能陷入内核执行 —— 对内核的所有访问都必须通过这些接口。

#### 进程家族树

Unix 系统的进程之间存在一个明显的继承关系，在linux系统中也是如此。所有的进程都是 PID 为 1 的 init 进程的后代。
内核在系统启动的最后阶段启动 init 进程。该进程读取系统的初始化脚本(initscript) 并执行其它的相关程序，最终完成系统启动的整个过程。

系统中的每个进程必有一个父进程，每个进程也可以拥有零个或多个子进程。拥有同一个父进程的所有进程都被称为**兄弟**。进程的关系存放在进程描述符中。每个 
`task_struct` 都包含一个指向其父进程的 `task_struct`、叫做 `parent` 的指针，还包含一个称为 `children` 的子进程链表。所以，对于
当前进程，可以通过下面的代码获得其父进程的进程描述符：
```c
struct task_struct*         myParent = current->parent;
```
同样，也可以按以下方式依次访问子进程：
```c
struct task_struct* task;
struct list_head* list;

list_for_each(list, &current->children) {
    task = list_entry(list, struct task_struct, sibling);
    /* task 现在指向当前的某个子进程 */
}
```
init 进程的描述符是作为 `init_task` 静态分配的。下面的代码可以很好的演示所有进程之间的关系：

```c
struct task_struct* task;
for (task = current; task != &init_task; task = task->parent) {
    // task 现在指向 init
}
```

实际上，你可以通过这种继承体系从系统的任何一个进程出发查找到任意指定的其它进程。但大多数时候，只需要通过简单的重复的方式就
可以遍历系统中的所有进程。这非常容易做到，因为一个任务队列本来就是一个双向的循环链表。对于给定的进程，获取链表中的下一个进程：
```c
list_entry(task->tasks.next, struct task_struct, tasks)
```
获取前一个进程的方法与之相同：
```c
list_entry(task->tasks.prev, struct tsak_struct, tasks)
```
这两个例程分别通过`next_task(task)`宏和`prev_task(task)`宏实现。而实际上，`for_each_process(task)`宏提供了依次访问整个任务队列的能力。
每次访问，任务指针都指向链表中的下一个元素：
```c
struct task_struct* task;

for_each_process(task) {
    // 它打印出每一个任务的名称和PID
    printk("%s [%d]\n", task->comm, task->pid);
}
```
> 在一个拥有大量进程的系统中通过重复来遍历所有进程的代价是很大的。因此，如果没有必须这样做的理由，不要这样做。

### 进程创建

Unix 的进程创建很特别。许多其它的操作系统都提供了产生(spawn)进程的机制，首先在新的地址空间里创建进程，读入可执行文件，最后开始执行。

Unix采用了与众不同的实现方式，它把上述步骤分解到两个单独的函数中去执行： `fork()` 和 `exec()`。

首先， `fork()` 通过拷贝当前进程创建一个子进程。子进程与父进程的区别仅仅在于 PID(每个进程唯一)、PPID(父进程的进程号，子进程将其设置为被拷贝进程的PID)
和某些资源和统计量(例如，挂起的信号，它没有必要被继承)。

`exec()`函数负责读取可执行文件并将其载入地址空间开始运行。把这两个函数组合起来使用的效果跟其它系统使用的单一函数的效果相似。

#### 写时拷贝

传统的 `fork()` 系统调用直接把所有资源复制给新创建的进程。这种实现过于简单且效率低下，因为它拷贝的数据也许并不共享，更糟糕的是，如果新进程打算立即执行一个新的映像，那么所有的拷贝都将前功尽弃。
Linux 的 `fork()` 十一鸥鸟更写时拷贝页实现。写时拷贝是一种可以推迟甚至免除拷贝数据的技术。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。

只有在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。也就是说，资源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享。这种技术使地址空间上的页的拷贝被推迟到实际发生写入的时候才进行。
在页根本不会被写入的情况下(距离来说，`fork()`后立即调用`exec()`)他们就无需复制了。

`fork()`的实际开销就是复制父进程的页表以及给子进程创建唯一的进程描述符。在一般情况下，进程创建后会马上运行一个可执行文件，这种优化可以避免拷贝大量根本都不会被使用的数据(地址空间里常常包含数十兆的数据)。
由于Unix强调进程快速执行的能力，所以这个优化是很重要的。

#### `fork()`

Linux 通过 clone() 系统调用实现 `fork()`。这个调用通过一系列的参数标志来指明父、子进程需要共享的资源。
`fork()`、`vfork()`和`__clone()`库函数都根据各自需要的参数标志去调用 `clone()`，然后由 `clone()` 去调用 `do_fork()`。

`do_fork()`完成创建中的大部分工作(定义在 `kernel/fork.c` 中)，该函数调用 `copy_process()` 函数，然后让进程开始运行。 `copy_process()`完成以下主要工作：
1. 调用 `dup_task_struct()` 为新进程创建一个内核栈、`thread_info` 结构和 `task_struct`，这些值与当前进程的值相同。此时，子进程和父进程的描述符是完全相同的。
2. 检查并确保新创建这个子进程后，当前用户拥有的进程数目没有超出给它分配的资源的限制。
3. 子进程着手使自己与父进程区别开来。进程描述符内的许多成员都要被清零或设为初始值。那些不是继承而来的进程描述符成员，主要是统计信息。task_struct中的大多数数据都依然未被修改。
4. 子进程的状态被设置为 `TASK_UNINTERRUPTIBLE`，以保证它不会投入运行。
5. `copy_process()`调用 `copy_flags()` 以更新 `task_struct` 的 flags 成员。表明进程是否拥有超级用户权限的
`PF_SUPERPRIV`标志被清零。表明进程还没有调用 `exec()` 函数的 `PF_FORKNOEXEC` 标志被设置。
6. 调用 `alloc_pid()` 为新进程分配一个有效的 PID。
7. 根据传递给 `clone()` 的参数标志，`copy_process()`拷贝或共享打开的文件、文件系统信息、信号处理函数、进程地址空间和命名空间等。
在一般情况下，这些资源会被给定进程的所有线程共享；否则，这些资源对每个进程是不同的，因此被拷贝在这里。
8. 最后，`copy_process()`做收尾工作并返回一个指向子进程的指针。

再回到 `do_fork()`函数，如果 `copy_process()` 函数返回成功，新创建的子进程被唤醒并让其投入运行。内核有意选择子进程首先执行。因为一般子进程都会马上调用 `exec()`函数，这样可以避免写时拷贝的额外开销，如果父进程首先
执行的话，有可能会开始向地址空间写入。

#### `vfork()`

除了不拷贝父进程的页表项外，`vfork()`系统调用和`fork()`的功能相同。子进程作为父进程的一个单独的线程在它的地址空间里运行，父进程被阻塞，直到子进程退出或执行
`exec()`。子进程不能向地址空间写入。在过去的 3BSD 时期，这个优化是有意义的，那时并未使用写时拷贝页来实现
`fork()`。现在由于在执行`fork()`十引入了写时拷贝页而且明确了子进程先执行，`vfork()`的好处就仅限于不拷贝父进程的页表项了。
如果 Linux 将来`fork()`有了写时拷贝页表项，那么`vfork()`就彻底没用了。另外由于`vfork()`语义非常微妙(试想，如果`exec()`调用失败会发生什么)，
所以理想情况下，系统最后不要调用 `vfork()`，内核也不用实现它。完全可以把 `vfork()` 实现成一个普普通通的 `fork()` —— 实际上，Linux2.2以前就是这么做的。
<br/>

`vfork()`系统调用的实现是通过向 `clone()` 系统调用传递一个特殊标志来进行的。
1. 在调用 `copy_process()` 时候，`task_struct` 的 `vfork_done`成员被设置为 `NULL`。
2. 在执行 `do_fork()` 时，如果给定特别标志，则 `vfork_done`会指向一个特定地址。
3. 子进程先开始执行后，父进程不是马上恢复执行，而是一直等待，直到子进程通过`vfork_done`指针向他发送信号。
4. 在调用 mm_release() 时候，该函数用于进程退出内存地址空间，并且检查 `vfork_done` 是否为空，如果不为空，则会向父进程发送信号。
5. 回到 `do_fork()`，父进程醒来并返回。
如果一切执行顺利，子进程在新的地址空间里运行而父进程也恢复了在原地址空间的运行。这样，开销确实降低了，不过它的实现并不是优良的。

### 线程在Linux中的实现

线程机制是现代编程技术中非常常用的一种抽象概念。该机制提供了在同一个程序内共享内存地址空间运行的一组线程。这些线程还可以共享打开的文件和其它资源线程机制支持并发程序设计技术，在多处理器系统上，它也能保证真正的并行处理。

Linux 实现线程的机制非常独特。从内核角度来说，它并没有线程这个概念。Linux把所有的线程都当作进程来实现。内核并没有准备特别的调度算法或者定义特别的数据结构来表征线程。
相反，线程仅仅被视为一个与其它进程共享某些资源的进程。每个线程都拥有唯一隶属于自己的 `task_struct`，素以在内核中，它看起来就像是一个普通的进程
(只是线程和其它一些进程共享某些资源，如地址空间)。

上述线程机制的实现与`Windows` 或是 `Sun Solaris` 等操作系统的实现差异非常大。这些系统都在内核中提供了专门支持线程的机制(这写系统常常把线程称作轻量级进程)。
“轻量级进程”这种叫法本身就概括了Linux在此处与其它系统的差异。

在其它系统中，相较于重量级的进程，线程被抽象成一种耗费较少资源，运行迅速的执行单元。而对于linux来说，它只是一种进程间共享资源的手段(Linux进程本身就够轻量级的来)。
举个例子来说，假如我们有一个包含四个线程的进程，在提供专门线程支持的系统中，通常会有一个包含指向四个不同线程的指针的进程描述符。
该描述符负责描述像地址空间、打开的文件这样的共享资源。线程本身再去描述它独占的资源。相反，Linux仅仅创建四个进程并分配四个普通的
`task_struct`结构。建立这四个进程时候指定他们共享某些资源，这是相当高雅的做法。

#### 创建线程

线程的创建和普通进程的创建类似，只不过在调用 `clone()` 的时候需要传递一些参数标志来指明需要共享的资源：
```c
clone(CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND, 0);
```
上面的代码产生的结果和调用`fork()`差不多，只是父子俩共享地址空间、文件系统资源、文件描述符和信号处理程序。换个说法就是，新建的进程和它的父进程就是流行的所谓线程。

对比以下，一个普通的 `fork()` 的实现是：
```c
clone(CLONE_VFORK|CLONE_VM|SIGCHLE, 0);
```

传递给 `clone()` 的参数标志决定了新创建进程的行为方式和父子进程之间共享的资源种类。
表3-1列举了这些`clone()`用到的参数标志以及他们的作用，这些是在`<linux/sched.h>`中定义的。

|参数标志|含义|
|:---|:---|
|CLONE_FILES|父子进程共享打开的文件|
|CLONE_FS|父子进程共享文件系统信息|
|CLONE_IDLETASK|将PID设置为0(只供idle进程使用)|
|CLONE_NEWNS|为子进程创建新的命名空间|
|CLONE_PARENT|指定子进程与父进程拥有同一个父进程|
|CLONE_PTRACE|继续调试子进程|
|CLONE_SETTID|将 TID 回写至用户空间|
|CLONE_SETTLS|为子进程创建新的 TLS(thread-local storage)|
|CLONE_SIGHAND|为父进程共享信号处理函数及被阻断的信号|
|CLONE_SYSVSEM|父子进程共享 System V SEM_UNDO 语义|
|CLONE_THREAD|父子进程放入相同的线程组|
|CLONE_VFORK|调用`vfork()`，所以父进程准备睡眠等待子进程将其唤醒|
|CLONE_UNTRACED|防止跟踪进程在子进程上强制执行 `CLONE_PTRACE`|
|CLONE_STOP|以TASK_STOPPED状态开始进程|
|CLONE_CHILD_CLEARTID|清除子进程的TID|
|CLONE_CHILD_SETTID|设置子进程的TID|
|CLONE_PARENT_SETTID|设置父进程的TID|
|CLONE_VM|父子进程共享地址空间|

#### 内核线程

内核经常需要在后台执行一些操作。这种任务可以通过内核线程完成 —— 独立运行在内核空间的标准进程。
<br/>

内核线程与普通进程间的区别在于内核线程没有独立的地址空间(实际上指向地址空间的mm指针被设置为NULL)。
它们只在内核空间运行，从来不切换到用户空间去，内核进程和普通进程一样，可以被调度，也可以被抢占。

> Linux确实会把一些任务交给内核线程去做，像`flush`和`ksofirqd`这些任务就是明显的例子。

内核线程是在系统启动时候由其它内核线程创建。内核是通过从kthreadd内核进程中衍生出所有新的内核线程来自动处理这一点的。在
`<linux/kthread.h>`中声明有接口，于是，从现有内核线程中创建一个新的内核线程的方法如下：
```c
struct task_struct* kthread_create (int (*threadfn) (void* data), void* data, const char namefmt[], ...)
```
新的任务是由 kthread 内核进程通过 `clone()`系统调用而创建的。新的进程将运行 `threadfn` 函数，给其传递的参数是data。
进程会被命名为 namefmt, namefmt接受可变参数列表类似于 `printf()` 的格式化参数。新创建的进程处于不可运行状态，如果不通过调用 
`wake_up_process()`明确地唤醒它，它不会主动运行。创建一个进程并让它运行起来，可以通过调用 `kthread_run()`来达到：
```c
struct task_struct* kthread_run (int (*threadfn) (void* data), void* data, const char namefmt[], ...)
```
这个例程是以宏实现的，只是简单地调用了 `kthread_create()` 和 `wake_up_process()`：
```c
#define kthread_run(threadfn, data, namefmt, ...)               \
({                                                              \
    struct task_struct* k;                                      \
    k = kthread_create(threadfn,data,namefmt,## __VA_ARGS__);   \
    if (!IS_ERR(k))                                             \
        wake_up_process(k);                                     \
    k;                                                          \
})
```
内核线程启动后就一直运行直到调用 `do_exit()` 退出，或者内核的其它部分调用 `kthread_stop()`退出，传递给
`kthread_stop()`的参数为`kthread_create()`函数返回的 `task_struct`结构的地址：
```c
int kthread_stop(struct task_struct* k);
```

### 进程终结

进程终结时候，内核必须释放它所占有的资源，并把这一消息告知父进程。

一般来说，进程的析构是自身引起的。它发生在进程调用`exit()`系统调用时候，既可能显示地调用这个系统调用，也可能显示地从某个程序的主函数返回
(其实C语言编译器会在main()函数的返回点后面放置调用`exit()`的代码)。当进程接受到它既不能处理也不能忽略的信号或异常时候，它还可能主动被终结。不管进程是怎么终结的，该任务
大部分要靠`do_exit()`(定义于kernel/exit.c)来完成，它要做下面这些繁琐工作：
1. 将 `task_struct` 中的标志成员设置为 `PF_EXITING`
2. 调用 `del_timer_sync()` 删除任一内核定时器。根据返回的结果，它确保没有定时器在排队，也没有定时器处理程序在运行。
3. 如果 BSD 的进程记账功能是开启的，`do_exit()` 调用 `acct_update_integrals()` 来输出记账信息。
4. 然后调用 `exit_mm()`函数释放进程占用的 `mm_struct`，如果没有别的进程使用它们(也就是说，这些地址空间没有被共享)，就彻底释放它们。
5. 接下来调用 `sem_exit()` 函数。如果进程排队等候 IPC 信号，它则离开队列。
6. 调用 `exit_files()` 和 `exit_fs()`，以分别递减文件描述符、文件系统数据的引用计数。如果某个引用计数值为0，那么就代表没有进程在使用相应的资源，此时可以释放。
7. 接着把存放在 `task_struct` 的 `exit_code` 成员中的任务退出代码置为由 `exit()` 提供的退出代码，或者去完成任何其它由内核机制规定的退出动作。退出代码存放在这里供父进程随时检索。
8. 调用 `exit_notify()` 向父进程发送信号，给子进程重新找养父，养父为线程组中的其它线程或者为 `init` 进程，并把进程状态(存放在`task_struct`结构的
`exit_state`中)设成 `EXIT_ZOMBIE`。
9. `do_exit()`调用 `schedule()` 切换到新的进程。因为处于 `EXIT_ZOMBIE` 状态的进程不会再被调度，所以这时进程所执行的最后一段代码。`do_exit()` 永不返回。

至此，与进程相关联的所有资源都被释放掉了(假设该进程是这些资源的唯一使用者)。进程不可运行(实际上也没有地址空间让他运行)并处于`EXIT_ZOMBIE`
退出状态。它占用的所有内存就是内核栈、thread_info结构和task_struct结构。此时进程存在的唯一目的就是向它的父进程提供信息。父进程检索到信息后，或者通知内核那是无关的信息后，由进程所持有的剩余内存被释放，归还给系统使用。

#### 删除进程描述符

在调用了 `do_exit()` 之后，尽管线程已经僵死不能再运行了，但是系统还是保留了它的进程描述符。前面说过，这样做可以让系统有办法在子进程终结后仍然获得它的信息。因此，进程终结时候所需的清理工作和进程描述符的删除被分开执行。
在父进程获得已终结的子进程的信息后，或者通知内核它并不关注那些信息后，子进程的`task_struct`结构才被释放。
<br/>

`wait()`这一族函数都是通过唯一(但是很复杂)的一个系统调用 `wait4()`来实现的。它的标准动作是挂起调用它的进程，直到其中的一个子进程退出，此时函数会返回该子进程的PID。
此外，调用该函数时候提供的指针会包含子函数退出时候的退出码。
<br/>

当最终需要释放进程描述符时候，`release_task()`会被调用，用以完成以下工作：
1. 它调用 `__exit_signal()`，该函数调用`__unhash_process()`，后者又调用 `detach_pid()` 从 `pidhash` 上删除该进程，同时也要从任务列表中删除该进程。
2. `_exit_signal()`释放目前僵死进程所使用的所有剩余资源，并进行最终统计和记录。
3. 如果这个进程是线程组最后一个进程，并且领头进程已经死掉，那么 `release_task()` 就要通知僵死的领头进程的父进程。
4. `release_task()` 调用`put_task_struct()`释放进程内核栈和`thread_info`结构所占的页，并释放`task_struct`所占的 slab 高速缓存。

至此，进程描述符和所有进程独享的资源全部被释放掉。

#### 孤儿进程造成的进退维谷

如果父进程在子进程之前退出，必须有机制来保证子进程能找到一个新的父亲，否则这些成为孤儿的进程就会在退出时候永远处于僵死状态，白白地耗费内存。
前面的部分已经有所暗示，对于这个问题，解决方法是给子进程在当前线程组内找一个线程作为父亲，如果不行，就让 init 做它们的父进程。在 `do_exit()`
中会调用 `exit_notify()`，该函数会调用 `forget_original_parent()`，而后者会调用 `find_new_reaper()` 来执行寻父过程：
```c
static struct task_struct* find_new_reaper (struct task_struct* father)
{
    struct pid_namespace* pid_ns = task_active_pid_ns (father);
    struct task_struct* thread;
    
    thread = father;
    while_each_thread(father, thread) {
        if (thread->flags & PF_EXITING)
            continue;
        if (unlikely(pid_ns->child_reaper == father))
            pid_ns->child_reaper = thread;
        return thread;
    }
    
    if (unlikely (pid_ns->child_reaper == father)) {
        write_unlock_irq (&tasklist_lock);
        if (unlikely(pid_ns == &init_pid_ns))
            panic ("Attempted to kill init!");
        zap_pid_ns_processes (pid_ns);
        write_lock_irq(&tasklist_lock);
        
        pid_ns->child_reaper = init_pid_ns.child_reaper;
    }
    return pid_ns->child_reaper;
}
```

这段代码试图找到进程所在线程组内的其它进程。如果线程组内没有其它进程，他就找到并返回的是 init 进程。现在，给子进程找到合适的养父进程来，
只需遍历所有子进程并为它们设置新的父进程：
```c
reaper = find_new_reaper(father);
list_for_each_entry_safe(p, n, &father->children, sibling) {
    p->real_parent = reaper;
    if (p->parent == father) {
        BUG_ON(p->ptrace);
        p->parent = p->real_parent;
    }
    reparent_thread(p, father);
}
```
然后调用 `ptrace_exit_finish()` 同样进行新的寻父过程，不过这次是给 `ptraced` 的子进程寻找父亲。
```c
void exit_ptrace(struct task_struct* tracer)
{
    struct task_struct *p, *n;
    LIST_HEAD(ptrace_dead);
    
    write_lock_irq(&tasklist_lock);
    list_for_each_entry_safe(p, n, &tracer->ptraced, ptrace_entry) {
        if (__ptrace_detach(tracer, p)) {
            list_add (&p->ptrace_entry, &ptrace_dead);
        }
    }

    write_unlock_irq(&tasklist_lock);
    BUG_ON(!list_empty(&tracer->ptraced));

    list_for_each_entry_safe(p, n, &ptrace_dead, ptrace_entry) {
        list_del_init(&p->ptrace_entry);
        release_task(p);
    }
}
```
这段代码遍历了两个链表：子进程链表和`ptrace`子进程链表，给每个子进程设置新的父进程。这两个链表同时存在的原因很有意思，它也是2.6内核的一个新特性。
当一个进程被跟踪时，它的临时父亲设定为调试进程。此时如果它的父进程退出了，系统会为它和它的所有兄弟重新找一个父进程。在以前的内核中，这就需要遍历系统所有
的进程来找这些子进程。现在的解决方法是在一个单独的被`ptrace`跟踪的子进程链表中搜索相关的兄弟进程 —— 用两个相对较小的链表减轻了遍历带来的消耗。
<br/>

一旦系统为进程成功地找到和设置了新的父进程，就不会再有出现驻留僵死进程的危险了。`init`进程会例行调用`wait()`来检查其子进程，清除所有与其相关的僵死进程。

### 小结

本章主要考察了系统中的核心概念 —— 进程。讨论了进程的一般特性，它为何如此重要，以及进程和线程之间的关系
<br/>

讨论了Linux如何存放和表示进程(`task_struct`和`thread_info`)，如何创建进程(通过 `fork()`，实际上最终是 `clone()`)，如何把新的执行镜像装入到地址空间
(通过 `exec()` 系统调用族)，如何表示进程的层次关系，父进程又是如何收集其后代的信息(通过 `wait()` 系统调用族)，以及进程最终如何消亡
(强制或者自愿地调用`exit()`)。进程是一个非常基础、非常关键的抽象概念，位于每一种现代操作系统的核心位置，也是我们拥有操作系统(用来运行程序)的最终原因。

## 进程调度

上边讲到的**进程**，他在操作系统看来是程序的运行态表现形式。接下来讨论进程调度，它是确保进程能有效工作的一个内核子系统。
<br/>

调度程序负责决定将哪个进程投入运行，何时运行以及运行多长时间。进程调度程序(常常简称调度程序)可看做在可运行态进程之间分配有限的处理器事件资源
的内核子系统。调度程序是像Linux这样的多任务操作系统的基础。只有通过调度程序的合理调度，系统资源才能最大限度地发挥作用，多进程才会有并发执行的效果。
<br/>

调度程序没有太复杂的原理。最大限度地利用处理器时间的原则是，只要有可以执行的进程，那么就总会有进程正在执行。但是只要系统中可运行的进程数目比处理器
的个数多，就注定某一个给定时刻会有一些进程不能执行。这些进程在等待运行。在一组处于可运行状态的进程中选择一个来执行，是调度程序所需完成的基本工作。

### 多任务

多任务操作系统就是能同时并发地交互执行多个进程的操作系统，但是这些进程并不都处于运行状态。
<br/>

多任务系统可以分为两类：**非抢占式多任务** 和 **抢占式多任务**。像所有 Unix 的变体和许多其它现代操作系统一样，Linux提供了抢占式的多任务模式。
在此模式下，由调度程序来决定什么时候停止一个进程的运行，以便其它进程能够得到执行机会。这个强制的挂起动作就叫做**抢占**。进程在被抢占之前能够运行的时间
是预先设置好的，而且有一个专门的名字，叫做**进程的时间片**。时间片实际上就是分配给每个可运行进程的处理器时间段。有效管理时间片能使调度程序从系统全局的
角度作出调度决定，这样做还可以避免个别进程独占系统资源。当今众多现代操作系统对程序运行都采用了动态时间片计算的方式，并且引入了可配置的计算策略。不过我们
将看到，Linux独一无二的“公平”调度程序本身并没有采取时间片来达到公平调度。
<br/>

相反，在非抢占式多任务模式下，除非进程自己主动停止运行，否则它会一直执行。进程主动挂起自己的操作称为**让步**。理想情况下，进程通常作出让步，以便让每个可运行进程享有足够的处理器时间。但这种机制有很多缺陷：
调度程序无法对每个进程该执行多长时间作出统一规定，所以进程独占的处理器时间可能超出用户的预料；更糟的是，一个决不做出让步的悬挂进程就能使系统崩溃。
Unix从一开始采用的就是抢先式的多任务，Mac OS 9(以及其前身)、Windows3.1(以及其前身)这些不是。

### Linux的进程调度

从 1991 年 Linux 的第一版到后来的 2.4 内核系列，Linux 的调度程序都相当简陋，设计近乎原始。当然它容易理解，但是它在众多可运行进程或者多处理器的环境下都
难以胜任。
<br/>

正因为如此，在 Linux2.5 开发系列的内核中，调度程序做了大手术。开始采用一种叫做 O(1) 调度程序的新调度程序 —— 它是因为其算法的行为而得名。它解决了先前版本
Linux 调度程序的许多不足，引入了许多强大的新特性和性能特征。这里主要要感谢静态时间片算法和针对每一处理器的运行队列，它们帮助我们摆脱了先前调度程序设计上的限制。
<br/>

O(1)调度器虽然在拥有数以十计(不是数以百计)的多处理器环境下尚能表现出近乎完美的性能和可扩展性，但是时间证明该调度算法对于调度那些响应时间敏感的程序却
有一些先天不足。这些程序我们称其为交互进程 —— 它无疑包括了所有需要用户交互的程序。正因为如此，O(1)调度程序虽然对于大服务器的工作负载很理想，但是在有
很多交互程序要运行的桌面系统上则表现不佳，因为其缺少交互进程。自 2.6 内核系统开发初期，开发人员为了提高对交互程序的调度性能引入了新的进程调度算法。
其中最为著名的是**“反转楼梯最后期限调度算法(RSDL)”**，该算法吸取了队列理论，将公平调度的概念引入了Linux调度程序。并且最终在 2.6.23 内核版本中替代了 
O(1) 调度算法，它此刻被称为 “完全公平调度算法”，简称 CFS。
<br/>

接下来讲解调度程序设计的基础和完全公平调度程序如何运用、如何设计、如何实现以及与它相关的系统调用。我们当然也会讲解O(1)调度程序，因为它毕竟是
经典Unix调度程序模型的实现方式。

### 策略

策略决定调度程序在何时让什么进程运行。调度器的策略往往就决定系统的整体印象，并且，还要负责优化使用处理器时间。无论从哪个方面来看，它都是至关重要的。

#### I/O消耗型和处理器消耗型的进程

进程可被分为**I/O消耗型**和**处理器消耗型**。前者指进程的大部分时间用来提交 I/O 请求或是等待 I/O 请求。因此，这样的进程经常处于可运行状态，但通常都是运行短短的一会儿，因为
它在等待更多的I/O请求时候最后总会阻塞(这里所说的I/O是指任何类型的可阻塞资源，比如键盘输入，或者是网络I/O)。举例来说，多数用户图形界面程序(GUI)
都属于I/O密集型，即便它们从不读取或者写入磁盘，它们也会在多数时间里都在等待来自鼠标或者键盘的用户交互操作。
<br/>

相反，处理器耗费型进程把时间大多用在执行代码上。除非被抢占，否则它们通常都一直不停的运行，因为它们没有太多的I/O需求。但是，因为它们不属于
I/O驱动型，所以从系统响应速度考虑，调度器不应该经常让它们运行。对于这类处理器消耗型的进程，调度策略往往是尽量降低它们的调度频率，而延长其运行时间。处理器消耗型进程的
极端例子就是无线循环地执行。更具代表性的例子就是那些执行大量数学计算的程序，如 sshkeygen 或者 MATLAB。
<br/>

当然，这种划分方法并非是绝对的。进程可以同时展示这两种行为：比如，X Window 服务器既是 I/O 消耗型，也是处理器消耗型。
还有些进程可以是I/O消耗型，但不属于处理器消耗型活动范围。其典型例子就是字处理器，其通常以等待键盘输入。但在任一时刻可能又粘住处理器疯狂地进行
拼写检查或者宏计算。
<br/>

调度撤略通常需要在两个矛盾的目标中寻找平衡：进程响应迅速(响应时间短)和最大系统利用率(高吞吐量)。为了满足上述需求，调度程序通常采用一套非常复杂的
算法来决定最值得运行的进程投入运行，但是它往往并不保证低优先级进程会被公平对待。Unix系统的调度程序更倾向于 I/O 消耗型程序，以便提供更好的程序
响应速度。Linux为了保证交互式应用和桌面系统的性能，所以对进程的响应做了优化(缩短响应时间)，更倾向于有限调度I/O消耗型进程。虽然如此，但在下面你会看到，
调度程序也并未忽略处理器消耗型的进程。
<br/>

#### 进程优先级

调度算法中最基本的一类就是基于优先级的调度。这时一种根据进程的价值和其对处理器时间的需求来对进程分级的想法，通常做法是(其并未被Linux系统完全采用)
优先级高的进程先运行，低的后运行，相同优先级的进程按轮询的方式进行调度(一个接一个，重复进行)。在某些系统中，优先级高的进程使用的时间片也较长。调度程序总是选择时间片未用尽而且优先级最高的进程运行。用户和系统都可以通过设置进程的优先级来影响系统的调度。
<br/>

Linux采用了两种不同的优先级范围。第一种是 **nice值**，它的范围是从 **-20** 到 **+19**，默认值为0；越大的nice值意味着更低的优先级(nice意味着你对其它系统中的进程更"优待"")。
相比高 nice 值的进程，低 nice 值的进程可以获得更多的处理器时间。nice 值是所有 Unix 系统中的标准化概念——但不同的 Unix 系统由于调度算法不同，因此 nice 值的运用方式有所差异。比如一些基于Unix的操作系统，
如 Mac OS X，进程的 nice 值代表分配给进程的时间片的绝对值；而 Linux 系统中，nice 值则代表了时间片的比例。
<br/>

第二种范围是**实时优先级**，其值的可配置的，默认情况下它的变化范围是 0 到 99(包括 0 和 99).与 nice 值意义相反，越高的实时优先级数值意味着进程优先级越高。
任何实时进程的优先级都高于普通进程，也就是说实时优先级和nice优先级处于互补相交的两个范畴。Linux 实时优先级的实现参考了 Unix 相关标准 —— 特别是 POSIX.1b。
大部分现代的 Unix 操作系统也都提供类似的机制。你可以通过命令：
```shell script
ps -eo state,uid,pid,ppid,rtprio,time,comm
```
查看你系统中的进程列表，以及它们对应的实时优先级(位于RTPRIO列下)，其中如果有进程对应列显示 “-”，则说明它不是实时进程。

#### 时间片

时间片是一个数值，它表明进程在被抢占前所能持续运行的时间。调度策略必须规定一个默认的时间偏，但这并不是件简单的事情。时间片过长会导致系统对交互的
响应表现欠佳，让人觉得系统无法并发执行应用程序；时间片太短会明显增大进程切换带来的处理器耗时，因为肯定会有相当一部分系统时间用在进程切换上，而这些进程能够用来运行的时间片却很短。此外，I/O
消耗型和处理器消耗型的进程之间的矛盾在这里也再次显露出来；I/O消耗型不需要长的时间片，而处理器消耗型的进程则希望越长越好(比如这样可以让它们的高速缓存命中率更高)。
<br/>

从上面的争论中可以看出，任何长时间片都将导致系统交互表现欠佳。很多操作系统中都特别重视这一点，所以默认的时间片很短，如 10ms。但是
Linux 的 CFS 调度器并没有直接分配时间片到进程，它是将处理器的使用比例划分给了进程。这样一来，进程所获的的处理器时间其实是和系统负载密切相关的。
这个比例进一步还会受进程 nice 值的影响， nice 值作为权重将调整进程所使用的处理器时间使用比。具有更高 nice 值(更低优先权) 的进程将被赋予低权重，从而
丧失了一小部分的处理器使用比；而具有更小nice值(更高优先级)的进程则会被赋予高权重，从而抢得更多的处理器使用比。
<br/>

像前面所说的，Linux系统是抢占式的。当一个进程进入可运行态，他就被准许投入运行。在多数操作系统中，是否要将一个进程立刻投入运行(也就是抢占当前进程)，
是完全由进程优先级和是否有时间片决定的。而在 Linux 中使用新的 CFS 调度器，其抢占时机取决于新的可运行程序消耗了多少处理器使用比。如果消耗的使用比比当前进程小，则新
进程立刻投入运行，抢占当前进程。肉则，将推迟其运行。

#### 调度策略的活动

想象下面这样一个系统，它拥有两个可运行的进程：一个文字编辑程序和一个视频编码程序。文字编辑程序很明显是 I/O 消耗型的，因为大多数时间都在等待用户的键盘输入
(无论用户的输入速度有多快，都不可能赶上处理的速度)。用户总是希望按下键系统能马上响应。

相反，视频编码程序是处理器消耗型的。除了最开始从磁盘上读出原始数据流和最后把处理好的视频输出外，程序所有的时间都用来对原始数据进行视频编码，处理器
很轻易地被 100% 使用。它对什么时间开始运行并没有太严格的要求 —— 用户几乎分辨不出也并不关心它到底是立刻就运行还是半秒钟以后才开始的。当然，它完成的越早越好，至于所花时间并不是我们关注的主要问题。
<br/>

在这样的场景中，理想情况是调度器应该给予文本编辑器程序相比视频编码程序更多的处理器时间，因为它属于交互式应用。对于文本编辑器而言，我们有两个目标。第一是希望系统给它更多的处理器时间，这并非因为它需要更多的处理器时间
(其实它并不需要)，是因为我们我们希望在它需要时总能得到处理器；第二是我们希望文本编辑器能在其被唤醒时候(也就是当用户打字时候)抢占视频解码程序。
这样才能确保文本编辑器具有很好的交互性能，以便能响应用户输入。
<br/>

在多数操作系统中，上述目标达成是要依赖系统分配给文本编辑器比视频解码程序更高的优先级和更多的时间片，先进的操作系统可以自动发现文本编辑器是交互型程序，
从而自动地完成上述分配动作。
<br/>

Linux操作系统同样需要追求上述目标，但是它采用不同的方法。它不再通过给文本编辑器分配给定的优先级和时间片，而是分配一个给定的处理器使用比。加入文本编辑器和视频编码程序是仅有的两个运行程序，并且又具有同样的 nice 值，
那么处理器的使用比将都是 50% —— 它们平分了处理器时间。但是因为文本编辑器将更多的时间用于等待用户输入，因此它肯定不会用到处理器的 50%。同时，
视频解码程序无疑将能有机会用到超过 50% 的处理器时间，以便它能更快速的完成解码任务。
<br/>

这里关键的问题是，当文本编辑器程序被唤醒的时候将会发生什么。我们首先目标是确保其能在用户输入发生时候立刻运行。在上述场景中，一旦文本编辑器被唤醒，
CFS发现文本编辑器比视频解码器运行的时间短的多。这种情况下，为了兑现让所有进程都能公平分享处理器的承诺，它会立即抢占视频解码程序，让文本编辑器
投入运行。文本编辑器运行后，立即处理了用户的击键输入后，又一次进入睡眠等待用户下一次输入。因为文本编辑器并没消费掉承诺给它的50%处理器使用比，因此情况依旧，CFS总会毫不犹豫的让文本编辑器在需要时候被投入运行，而让视频处理程序
只能在剩余下的时刻运行。

### Linux 调度算法

在前面内容中，我们抽象地讨论了进程调度原理，只是偶尔提及 Linux 如何把给定的理论应用到实际中。在已有的调度原理基础上，我们进一步探讨具有Linux特色的进程调度程序。

#### 调度器类

Linux 调度器是以模块方式提供的，这样做的目的是运行不同类型的进程可以又针对性的选择调度算法。
<br/>

这种模块化结构被称为**调度器类**，它允许多种不同的可动态添加的调度算法并存，调度属于自己范畴的进程。每个调度器都有一个优先级，基础的调度器代码定义在
`kernel/sched.c`文件中，它会按照优先级顺序遍历调度类，拥有一个可执行进程的最高优先级的调度器类胜出，去选择下面要执行的那一个程序。
<br/>

完全公平调度(CFS)是一个针对普通进程的调度类，在 Linux 中称为 SCHED_NORMAL (在 POSIX 中称为 SCHED_OTHER)，CFS 算法实现定义在文件 `kernel/sched_fair.c`
中。本节下面的内容将重点介绍 CFS 算法。

#### Unix系统中的进程调度

在讨论公平调度算法前，我们必须首先认识一下传统 Unix 系统的调度过程。正如前面所述，现代进程调度器有两个通用的概念：进程优先级和时间片。时间片是指进程运行多少时间，
进程一旦启动就会有一个默认时间片。具有更高优先级的进程将运行的更频繁，而且(在多数系统上)也会被赋予更多的时间片。在 Unix 系统上，优先级以 nice 值形式
输出给用户空间。这点听起来简单，但是在现实中，却会导致许多反常的问题，我们下面具体讨论。
<br/>

**第一个问题**，若要将 nice 值映射到时间片，就必然需要将 nice 单位值对应到处理器的绝对时间。但这样做将导致进程切换无法最优化进行。举例说明，
假定我们将默认 nice 值(0) 分配给一个进程 —— 对应的是一个 100ms 的时间片：同时再分配一个最高 nice 值(+20，最低的优先级)给另一个进程 —— 
对应的时间片是 5ms。我们接着假定上述两个进程都处于可运行状态。那么默认优先级的进程将获得 20/21 (105ms中的100ms) 的处理器时间，而低优先级的进程会获得
1/21(105ms中的5ms)的处理器时间。我们可以选择任意数值用于本例子中。现在，我们看看如果运行两个同等低优先级的进程的情况将如何。我们是希望它们能各自获得
一半的处理器时间，事实上也确实如此。但是另一个进程每次只能获得 5ms 的处理器时间(10ms中各占一般)。也就是说，相比刚才例子中 105ms 内进行一次上下文切换，现在则需要在 10ms 内继续进行两次上下文切换。类推，如果是两个具有普通优先级的进程，它们同样会每个获得 50% 的处理器时间，但是是在 100ms 内各获得一般。显然，我们看到这些时间片的分配方式并不很理想：它们是给定
nice 值到时间片映射与进程运行优先级混合的共同作用结果。事实上，给定高 nice 值(低优先级)的进程往往是后台进程，且多是计算密集型；而普通优先级的进程则更多的是前台用户任务。所以这种时间片分配方式显然是和初衷背道而驰的。
<br/>

**第二个问题** 设计相对 nice 值，同时和前面的 nice 值到时间片映射关系也脱不了关系。假设我们有两个进程，分别具有不同的优先级。第一个假设 nice 值只是0，
第二个假设是1。它们将被分别映射到时间片 100ms 和 95ms (O(1)调度算法确实就这么干类)。它们的时间片几乎一样，差别微乎其微。但是如果我们的进程分别赋予 18 和 19
的 nice 值，那么它们则分别被映射为 10ms 和 5ms 的时间片。如果这样，前者相比后者获得了两倍的处理器时间！不过 nice 值通常都使用相对值(nice系统调用是在原值上
增加或减少，而不是在绝对值上操作)，也就是说：“把进程的nice值减少1”所带来的效果极大的取决于其 nice 的初始值。
<br/>

**第三个问题** 如果执行 nice 值到时间片的映射，我们需要能分配一个绝对时间片，而且这个绝对时间片必须能在内核的测试范围内。在多数操作系统中，上述要求意味着时间片必须是
定时器节拍的整数倍。但是这么做必然会引发了几个问题。首先，最小时间片必然是定时器节拍的整数倍，也就是 10ms 或者 1ms 的倍数。

其次，系统定时器限制了两个时间片的差异：连续的 nice 值映射到时间片，其差别范围多至 10ms 或者 少则 1ms。最后，时间偏还会随着定时器节拍改变。
<br/>

**第四个问题** 也是最后一个关于基于优先级的调度器为了优化交互任务而唤醒相关进程的问题。这种系统中，你可能为了进程能更快的投入运行，而去对新要唤醒的进程提升优先级，即便它们的
时间片已经用尽了。虽然上述方法确实能提升不少交互性能，但是一些例外情况也有可能发生，因为它同时也给某些特殊的睡眠/唤醒用例一个玩弄调度器的后门，
使得给定进程打破公平原则，获得更多处理器时间，顺还系统中其它进程的利益。
<br/>

上述问题中的绝大多数都可以通过对传统 Unix 调度器进行改造解决，虽然这种改造修改不小，但是也并非是结构性的调整。比如，将 nice 值呈几何增加而非算数增加
的方式解决第二个问题：采用一个新的度量机制将从 nice 值到时间片的映射与定时器节拍分离开来，一次解决第三个问题。但是这些解决方案都回避了实质问题 —— 即
分配绝对的时间片引发的固定的切换频率，给公平性造成了很大变数。CFS采用的方法是对时间片分配方式进行根本性的重新设计(就进程调度器而言)：完全摒弃时间片
而是分配给进程一个处理器使用比重。这种方式下，CFS 确保了进程调度中能有恒定的公平性，而将切换频率置于不断变动中。

#### 公平调度

CFS 的出发点基于一个简单的理念：进程调度的效果应如同系统具备一个理想中的完美多任务处理器。在这种系统中，每个进程将获得 $1/n$ 的处理器时间 —— n 是指可运行进程的数量。
同时，我们可以调度给它们无限小的时间周期，所以在任何可测量周期内，我们给予 n 和进程中每个进程同样多的运行时间。距离说明，加入我们有两个运行进程，在标准 Unix
调度模型中，我们先运行其中一个 5ms，然后再运行另一个 5ms。但它们任何一个运行时候都将占有 100% 的处理器，它们各自使用处理器一半的能力。
<br/>

当然，上述理想模型并非显示，因为我们无法在一个处理器上真的同时运行多个进程。而且如果每个进程运行无限小的时间周期也是不高效的 —— 因为调度时进程抢占会带来一定的
代价：将一个进程换出，另一个换入本身有消耗，同时还会影响到缓存的效率。因此虽然我们希望所有进程能值运行一个非常短的周期，但是 CFS 充分考虑了这将带来的额外消耗，现实中首先要
确保系统新能不受损失。CFS的做法是允许每一个进程运行一段时间、循环轮转、选择运行最少的进程作为下一个运行进程，而不再采用分配给每个进程时间片的做法了，

CFS 在所有可运行进程总数基础上计算出一个进程应该运行多久，而不是依靠 nice 值来计算时间片。nice值在 CFS 中被作为进程获得的处理器运行比的权重，这是相对默认 nice 值
进程的进程而言的；相反，更低的 nice 值(越高的优先级)的进程获得更好的处理器使用权重。
<br/>

每个进程都按其权重在全部可运行进程中所占的 “时间片” 来运行，为了计算准确的时间片，CFS 为完美多任务中的无限小调度周期的近似值设立了一个目标。而这个目标称作 “目标延迟”，
越小的调度周期将带来越好的交互性，同时也更接近完美的多任务。但是你必须承受更高的切换代价和更差的系统总吞吐能力。让我们假定目标延迟值是 20ms，我们有两个同样优先级
的可运行任务(无论这些任务的优先级是多少)。每个任务在被其它任务抢占前运行 10ms，如果我们有 4 个这样的任务，则每个智能运行 5ms。进一步设想，
如果有 20 个这样的任务，那么每个仅仅只能获得 1ms 的运行时间。
<br/>

你一定注意到了，当可运行任务数量趋于无限时候，它们各自获得的处理器时间比和时间片都将趋于0.这样无疑造成了不可接收的切换消耗。CFS 为此引入每个进程获得的
时间片底线，这个底线称为最小粒度。默认情况下这个值是 1ms。如此依赖，即便是可运行进程数量趋于无穷，每个最少也能获得 1ms 的运行时间，确保切换消耗被限制在一定
范围内(敏锐的读者会注意到假如在进程数量变得非常多的情况下，CFS并非一个完美的公平制度，因为这时处理器时间片再小也无法突破最小粒度。的却如此，
尽管修改过的公平队列方法确实能提高这方面的公平性，但是 CFS 的算法本身其实已经决定在这方面折中了。但还好，因为通常情况下系统中只会有几百个可运行进程，无疑，
这时 CFS 的相当公平的)。
<br/>

现在，让我们再看看具有不同 nice 值的两个可运行进程的运行情况 —— 比如一个具有默认 nice 值(0)，另一个具有额 nice 值为 5.这些不同的 nice 值对应不同的权重，
所以上述两个进程将获得不同的处理器使用比。这个例子中，nice 值是 5 的进程的权重是默认 nice 进程的 1/3.如果我们的目标延迟是 20ms，那么这两个进程将分别获得 15ms 和 5ms 的
处理器时间。再比如我们的两个可运行进程的 nice 值分别是 10 和 15，它们分配的时间片将是多少呢？还是 15ms 和 5ms！可见，绝对的 nice 值不再影响调度决策：只有相对值才会影响处理器时间的分配比例。
<br/>

总结以下，任何进程所获得的处理器时间是由它自己和其它所有可运行进程 nice 值的相对差值决定的。nice值对时间片的作用不再是算数加权，而是几何加权。任何nice值对应的绝对时间不再是一个绝对值，而是处理器的使用比。
CFS 称为公平调度器是因为它确保给每个进程公平的处理器使用比。正如我们知道的，CFS 不是完美的公平，它只是近乎完美的多任务。但是它确实在多进程环境下，降低了调度延迟带来的不公平性。

### Linux 调度的实现

在讨论了采用 CFS 调度算法的动机和其内部逻辑后，我们现在可以开始具体探索 CFS 是如何得以实现的。其相关代码位于文件 `kernel/sched_fair.c` 中。我们将特别关注其四个组成部分：
- 时间记账
- 进程选择
- 调度器入口
- 睡眠和唤醒

#### 时间记账

所有的调度器都必须对进程运行时间做记账。多数 Unix 系统，正如我们前面所说，分配一个时间片给每一个进程。那么每次系统时钟节拍发生时候，时间片都被减少一个节拍周期。
当一个进程的时间片被减少到0时候，它就会被另一个尚未减到 0 的时间片可运行进程抢占。

##### 调度器实体结构

CSF 不再有时间片的概念，但是它也必须维护每个进程运行的时间记账，因为它需要确保每个进程只在公平分配给它的处理器时间内运行。CFS使用调度器实体结构
(定义在文件`<linux/sched.h>`的 `struct_sched_entity` 中)来追踪进程运行记账：
```c
struct sched_entity {
    struct load_weight              load;
    struct rb_node                  run_node;
    struct list_head                group_node;
    unsigned int                    on_rq;
    u64                             exec_start;
    u64                             sum_exec_runntime;
    u64                             vruntime;
    u64                             prev_sum_exec_runtime;
    u64                             last_wakeup;
    u64                             avg_overlap;
    u64                             nr_migrations;
    u64                             start_runtime;
    u64                             avg_wakeup;
    
    // 这里省略了很多统计变量，只有在设置了 CONFIG_SCHEDSTATS 时候才启用这些变量
};
```

调度器实体结构作为一个名为 se 的成员变量，嵌入在进程描述符 `struct task_struct` 内。
<br/>

##### 虚拟实时

vruntime 变量存放进程的虚拟运行时间，该运行时间(华仔运行上的时间和)的计算是经过了所有可运行进程总数的标准化(或者说是被加权的)。虚拟时间是以 ns 为单位的，
所以 vruntime 和定时器节拍不再相关。虚拟运行时间可以帮助我们逼近 CFS 模型所追求的 “理想多任务处理器”。如果我们真有这样一个理想的处理器，那么我们就不再需要
vruntime 了。因为优先级相同的所有进程的虚拟运行时间都是相同的 —— 所有任务都被接收到相等的处理器份额。但是因为处理器无法实现完美的多任务，它必须依次运行每个任务。
因此 CFS 使用 vruntime 变量来记录一个程序到底运行了多长时间以及它还应该再运行多久。

定义在 `kernel/sched_fair.c` 文件中的 `update_curr()` 函数实现了该记账功能：
```c
static void update_curr (struct cfs_rq* cfs_rq)
{
    struct sched_entity* curr = cfs_rq->curr;
    u64 now = rq_of(cfs_rq)->clock;
    unsigned long delta_exec;

    if (unlikely(!curr)) {
        return;
    }

    // 获得从最后依次修改负载后当前任务所占用的运行总时间(在 32 位系统上这不会溢出)
    delta_exec = (unsigned long) (now - curr->exec_start);
    if (!delta_exec)
        return;
    __update_curr(cfs_rq, curr, delta_exec);
    curr->exec_start = now;

    if (entity_is_task(curr)) {
        struct task_struct* curtask = task_of (curr);
        trace_shced_stat_runtime(curtask, delta_exec, curr->vruntime);
        cpuacct_charge(curtask, delta_exec);
        account_group_exec_runtime(curtask, delta_exec);
    }
}
```

`update_curr()` 计算了当前进程的执行时间，并且将其存放在变量 delta_exec 中。然后它又将运行时间传递给了 `__update_curr()`，由后者再根据当前可运行进程总数对运行时间进行加权计算。
最终将上述的权重值与当前运行进程的 vrunntime 相加。

```c
// 更新当前任务的运行时统计数据。跳过不在调度类中的当前任务
static inline void __update_curr (struct cfs_rq* cfs_rq, struct shced_entity* curr, unsigned long delta_Exec)
{
    unsigned long delta_exec_weighted;
    schedstat_set (curr->exec_max, max((u64)delta_exec,curr->exec_max));

    curr->sum_exec_runtime += delta_exec;
    schedstat_add (cfs_rq, exec_clock, delta_Exec);
    delta_exec_weighted = calc_delta_fair (delta_exec, curr);

    curr->vruntime += delta_exec_weighted;
    update_min_vruntime(cfs_rq);
}
```

`update_curr()`是由系统定时器周期性调用，无论在进程处于可运行状态，还是被阻塞处于不可运行状态。根据这种方式，vruntime 可以准确地测量给定进程的运行时间，
而且可直到谁应该是下一个被运行的进程。

#### 进程选择

在前面内容中我们的讨论中谈到若存在一个完美的多任务处理器。所有可运行进程的 vruntime 值将一致。但事实上我们没有找到完美的多任务处理器，因此 CFS 试图利用一个简单的
规则去均衡进程的虚拟运行时间：当 CFS 需要选择下一个运行时间时候，它会挑一个具有最小 vruntime 的进程。这其实就是 CFS 调度算法的核心：选择具有最小 vruntime 的任务。
那么剩下的内容我们就来讨论到底是如何实现选择具有最小 vruntime 值的进程。

CFS 使用红黑树来组织可运行进程队列，并利用其迅速找到最小 vruntime 值的进程

> 在 linux 中，红黑树被称为 btree，它是一个自平衡二叉搜索树。

##### 挑选下一个任务

我们先假设，有那么一个红黑树存储了系统中所有的可运行进程，其中节点的键值便是可运行进程的虚拟运行时间。稍后我们将看到如何生成该树，但现在我们假定已经拥有它来。CFS 调度器选取待运行的下一个进程，
是所有进程中 vruntime 最小的哪个，它对应的便是在树中最左侧的叶子节点。实现这一过程的函数是：`__pick_next_entity()`，它定义在文件 `kernel/sched_fair.c` 中：

```c
static struct sched_entity* __pick_next_entity(struct cfs* cfs_rq)
{
    struct rb_node* left = cfs_rq->rb_leftmost;
    
    if (!left)
        return NULL;

    return rb_entry(left, struct sched_entity, run_node);
}
```
> 注意 __pick_next_entity() 函数本身并不会遍历树找到最左叶子节点，因为该值已经缓存在 rb_leftmost 字段中。虽然红黑树让我们可以很有效地找到最左叶子节点(O(树的高度))等于
> 树节点总数的 O(log n)，这是平衡树的优势)，但是更容易的做法是把最左叶子节点缓存起来。这个函数的返回值便是 CFS 调度选择的下一个运行进程。如果该函数返回值是 NULL， 那么表示没有最
> 左叶子节点，那就是说树中没有任何节点了。这种情况下，表示没有可运行进程， CFS 调度器便选择 idle 任务运行。

##### 向树中加入进程

现在，我们来看 CFS 如何将进程加入 rbtree 中，以及如何缓存最左叶子节点。这一切发生在进程变为可运行状态(被唤醒)或者是通过`fork()`调用第一次创建进程时。`enqueue_entity()`函数实现了这一目的：

```c
static void enqueue_entity (struct cfs_rq* cfs_rq, struct sched_entity* se, int flags)
{
    // 通过调用 update_curr()，在更新 min_vruntime 之前先更新规范化的 vruntime
    if (!(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATE))
        se->vruntime += cfs_rq->min_vruntime;

    // 更新 当前任务 的运行时统计数据
    update_curr(cfs_rq);

    account_entity_enqueue(cfs_rq, se);

    if (flags & ENQUEUE_WAKEUP) {
        place_entity(cfs_rq, se, 0);
        enqueue_sleeper(cfs_rq, se);
    }

    update_stats_enqueue(cfs_rq, se);
    check_spread(cfs_rq, se);
    if (se != cfs_rq->curr)
        __enqueue_entity(cfs_rq, se);
}
```

该函数更新运行时间和其它一些统计数据，然后调用 `__enqueue_entity()` 进行繁重的插入操作，把数据项真正插入到红黑树中：
```c
// 把一个调度实体插入 红黑树中

static void __enqueue_entity (struct cfs_rq* cfs_rq, struct sched_entity* se)
{
    struct rb_node** link = &cfs_rq->tasks_timeline.rb_node;
    struct rb_node* parent = NULL;
    struct sched_entity* entity;
    
    s64 key = entity_key(cfs_rq, se);
    int leftmost = 1;

    while (*link) {
        parent = *link;
        entry = rb_entry(parent, struct sched_entity, run_node);
        // 我们并不关心冲突，具有相同键值的节点呆在一起
        if (key < entity_key(cfs_rq, entry)) {
            link = &parent->rb_left;
        } else {
            link = &parent->rb_right;
            leftmost = 0;
        }
    }

    // 维护一个缓冲，其中存放树最左叶子节点(也就是最长使用的)
    if (leftmost) {
        cfs_rq->rb_leftmost = &se->run_node;
    }
    rb_link_node(&se->run_node, parent, link);
    rb_insert_color(&se->run_node, &cfs_rq->tasks_timeline);
}
```

我们来看看上述函数，`while()`循环中遍历树以寻找合适的匹配键值，该值就是被插入进程的 vruntime。平衡二叉树的基本规则是，如果键值小于当前节点的键值，则需转向树的左分支；相反如果大于当前节点的键值，则需转向右分支。
如果一旦走过右边分支，哪怕一次，也说明插入的进程不会是新的最左节点，因此可以设置 leftmost 为 0.如果一致都是向左移动，那么 leftmost 维持 1，这说明
我们有一个新的最左节点，并且可以更新缓存 —— 设置 rb_leftmost 指向被插入的进程。当我们沿着一个方向和一个没有自节点的节点比较后：link如果这时是 NULL，循环随之终止。当退出循环后，接着在父节点上调用 `rb_link_node()`，
以使得新插入的进程成为其子节点。最后函数 `rb_insert_color()` 更新树的自平衡相关的属性。

##### 从树中删除进程



















